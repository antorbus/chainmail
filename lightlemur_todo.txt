The Great Todo List

*** python port *** (in progress)

*** unit tests *** 

*** memory leak tests ***

*** contiguous optimizations ***
TODO kernel forwards contiguous all the time, does not work for shape ops
once that is done, can make special kernels optimized for contiguous

*** u ops ***
    - neg
    - exp (done)
    - log
    - relu (done)
    - sqrt
    - abs
    - sign 
    - reciprocal
    - sigmoid (done)

*** b ops ***
    - add (done)
    - mul (done)
    - sub
    - div (done)
    - pow (done)
    - eq
    - neq
    - lt
    - gt
    - minimum 
    - maximum

*** r ops ***
    - sum (done)
    - max
    - min
    - mean (TODO must be done with sum + division)
    - argmax 
    - argmin
    
*** s ops ***
    - view
    - permute
    - expand (not done)
    - slice (further down the line)
    - flip  (further down the line)
    - as_strided (further down the line)
    - add more
    

*** cuda ***

*** matmul op ***
    - gpu kernel
    - metal kernel

*** deepcopy op*** a.deepcopy() -> b
*** contiguous op*** a.contiguous() ->
*** concat *** cat(a,b) -> c
*** split ***  split(a, size, dims) -> b,c,...
*** index ***  a[idx] -> b

*** optimizers ***
*** models ***
*** conv (matmul + im2col) ***
*** lazy execution (.compile()) ***
*** kernel fusion/compiler (.compute()) ***

*** other ***
update docs, next_seed0 MUST BE REUSED as seed. next_seed1 must be created
update docs, unary, binary, reduce all create new memory. Shape ops do not (caveat).
update docs, specific function checks should be done in the interface
stubs for interface
checks for forward_kernel etc
improve print (add limits)
using contiguous to improve binary performance
make file clean frontend/__pycache__?
add a small print of elements to the graph

*** compiler ***
- when making compiler, malloc/object pool all of the memory before hand so there is not dynamic allocation
- compiler adds the model you are using to a file with #defines for functions with known parameters so that all the sizes
of the loops are known beforeand and compile dynamically
- one thread per independen leaf node/expression. Keep an "object pool" of threads, allocate as needed.


NOTE
- check if a new tensor has to allocate new memory per function called (i.e. immutable)
answer: yes because when compiler fusion is done, this wont matter
- all B, O, R forward kernels all return contiguous tensors
- all B, O, backward kernels all return contiguous tensors

fwd
unary ops   :   cont/con't -> cont 
binary ops  :   cont/con't, cont/con't -> cont 
reduce ops  :   cont/con't -> cont 
shape       :   
                view    cont -> cont 
                expand  cont/con't -> con't
                permute cont/con't -> con't


todo:
think - is it worth it to make every operation contiguous? maybe yes because if not GPU tensors 
will be accessing non contiguous tensors all the way

answer:
did some tests, torch linear automatically makes tensor (linear) contiguous for GPU always 
because if not the model is A LOT SLOWER. true for permute and expand

make readme better, improve DOCS
add all methods torch has for tensor, like is_contiguous etc, update print to use these functions